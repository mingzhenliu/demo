#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
SpecKit å¢é‡æ›´æ–°è„šæœ¬ v2.0
ä»æ¨¡æ¿ä»“åº“å¢é‡æ›´æ–° SpecKit æ ¸å¿ƒæ–‡ä»¶
ç‰ˆæœ¬ï¼š2.0.0 | æ›´æ–°æ—¥æœŸï¼š2026-01-20
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

åŠŸèƒ½ï¼š
1. æ”¯æŒç›®å½•çº§åˆ«è‡ªåŠ¨åŒæ­¥é…ç½®
2. ä¸‰å±‚è¿‡æ»¤ä¿æŠ¤æœºåˆ¶ï¼ˆç›®å½•é»‘åå•ã€æ–‡ä»¶é»‘åå•ã€æœ¬åœ°è¦†ç›–ï¼‰
3. åˆ é™¤åå•åŠŸèƒ½
4. v1.0 é…ç½®å…¼å®¹ä¸è‡ªåŠ¨è¿ç§»
5. é¢„è§ˆæ¨¡å¼å’Œè¯¦ç»†æ—¥å¿—
"""

import argparse
import fnmatch
import hashlib
import json
import os
import shutil
import subprocess
import sys
import tempfile
from dataclasses import dataclass, field
from pathlib import Path
from typing import Dict, List, Optional, Tuple, Union


# ============================================================================
# é¢œè‰²å®šä¹‰
# ============================================================================

class Colors:
    RED = '\033[0;31m'
    GREEN = '\033[0;32m'
    YELLOW = '\033[1;33m'
    BLUE = '\033[0;34m'
    CYAN = '\033[0;36m'
    MAGENTA = '\033[0;35m'
    BOLD = '\033[1m'
    NC = '\033[0m'  # No Color


def print_info(message: str) -> None:
    print(f"{Colors.BLUE}[INFO]{Colors.NC} {message}")


def print_success(message: str) -> None:
    print(f"{Colors.GREEN}[OK]{Colors.NC} {message}")


def print_warning(message: str) -> None:
    print(f"{Colors.YELLOW}[SKIP]{Colors.NC} {message}")


def print_error(message: str) -> None:
    print(f"{Colors.RED}[ERROR]{Colors.NC} {message}")


def print_verbose(message: str, verbose: bool) -> None:
    if verbose:
        print(f"{Colors.CYAN}[DEBUG]{Colors.NC} {message}")


# ============================================================================
# æ•°æ®æ¨¡å‹
# ============================================================================

@dataclass
class ExclusionReason:
    """æ’é™¤åŸå› å¸¸é‡"""
    NOT_IN_WHITELIST = "ä¸åœ¨ç›®å½•ç™½åå•ä¸­"
    IN_EXCLUDE_DIR = "åœ¨ç›®å½•é»‘åå•ä¸­"
    IN_DELETE_LIST = "åœ¨åˆ é™¤åå•ä¸­"
    IN_BLACKLIST = "åœ¨æ–‡ä»¶é»‘åå•ä¸­"
    LOCAL_OVERRIDE = "æœ¬åœ°è¦†ç›–æ ‡è®°"
    NO_DIFFERENCE = "æ–‡ä»¶å†…å®¹ç›¸åŒ"
    NOT_EXIST_NEW_POLICY = "ç›®æ ‡æ–‡ä»¶ä¸å­˜åœ¨ä¸”æ–°æ–‡ä»¶ç­–ç•¥ç¦ç”¨"
    SYMLINK = "ç¬¦å·é“¾æ¥æ–‡ä»¶"


@dataclass
class FileToSync:
    """éœ€è¦åŒæ­¥çš„æ–‡ä»¶"""
    rel_path: str
    action: str  # create, update, delete, skip
    src: str = ""
    dst: str = ""
    reason: str = ""


@dataclass
class SyncConfig:
    """åŒæ­¥é…ç½®"""
    version: str = "2.0.0"
    sync_directories: List[Dict] = field(default_factory=list)
    exclude_directories: List[Dict] = field(default_factory=list)
    delete_list: List[str] = field(default_factory=list)
    file_blacklist: List[str] = field(default_factory=list)
    new_file_policy: Dict = field(default_factory=lambda: {"enabled": False})
    local_overrides: Dict = field(default_factory=lambda: {"enabled": True})
    backup: Dict = field(default_factory=lambda: {"enabled": True})


@dataclass
class SyncResult:
    """åŒæ­¥æ‰§è¡Œç»“æœ"""
    updated: int = 0
    skipped: int = 0
    created: int = 0
    deleted: int = 0
    excluded: int = 0
    errors: int = 0
    files: List[FileToSync] = field(default_factory=list)
    excluded_files: List[Tuple[str, str]] = field(default_factory=list)  # (path, reason)


# ============================================================================
# å·¥å…·å‡½æ•°
# ============================================================================

def get_repo_root() -> Path:
    """è·å– git ä»“åº“æ ¹ç›®å½•"""
    result = subprocess.run(
        ['git', 'rev-parse', '--show-toplevel'],
        capture_output=True,
        text=True
    )
    if result.returncode != 0:
        print_error("æ— æ³•è·å– git ä»“åº“æ ¹ç›®å½•")
        sys.exit(1)
    return Path(result.stdout.strip())


def get_script_dir() -> Path:
    """è·å–è„šæœ¬æ‰€åœ¨ç›®å½•"""
    return Path(__file__).resolve().parent


def normalize_path(path: str) -> str:
    """æ ‡å‡†åŒ–è·¯å¾„ï¼Œä½¿ç”¨ / ä½œä¸ºåˆ†éš”ç¬¦"""
    return path.replace(os.sep, '/')


def match_pattern(pattern: str, file_path: str) -> bool:
    """
    åŒ¹é…æ–‡ä»¶è·¯å¾„æ¨¡å¼ï¼ˆæ”¯æŒé€šé…ç¬¦ï¼‰

    Args:
        pattern: æ¨¡å¼å­—ç¬¦ä¸²ï¼Œæ”¯æŒ * é€šé…ç¬¦
        file_path: æ–‡ä»¶è·¯å¾„ï¼ˆæ ‡å‡†åŒ–åï¼Œä½¿ç”¨ / åˆ†éš”ï¼‰

    Returns:
        bool: æ˜¯å¦åŒ¹é…
    """
    # æ ‡å‡†åŒ–è·¯å¾„
    pattern = normalize_path(pattern)
    file_path = normalize_path(file_path)

    # å¦‚æœæ¨¡å¼ä¸åŒ…å«é€šé…ç¬¦ï¼Œç²¾ç¡®åŒ¹é…
    if '*' not in pattern and '?' not in pattern and '[' not in pattern:
        return file_path == pattern

    # ä½¿ç”¨ fnmatch è¿›è¡Œæ¨¡å¼åŒ¹é…
    return fnmatch.fnmatch(file_path, pattern)


def match_any_pattern(patterns: List[str], file_path: str) -> bool:
    """æ£€æŸ¥æ–‡ä»¶è·¯å¾„æ˜¯å¦åŒ¹é…ä»»ä¸€æ¨¡å¼"""
    for pattern in patterns:
        if match_pattern(pattern, file_path):
            return True
    return False


def compute_md5(file_path: Path) -> Optional[str]:
    """
    è®¡ç®—æ–‡ä»¶çš„ MD5 å“ˆå¸Œå€¼

    Args:
        file_path: æ–‡ä»¶è·¯å¾„

    Returns:
        MD5 å“ˆå¸Œå€¼ï¼Œå¦‚æœæ–‡ä»¶ä¸å­˜åœ¨æˆ–è¯»å–å¤±è´¥è¿”å› None
    """
    if not file_path.exists() or not file_path.is_file():
        return None

    try:
        hash_md5 = hashlib.md5()
        with open(file_path, 'rb') as f:
            for chunk in iter(lambda: f.read(4096), b""):
                hash_md5.update(chunk)
        return hash_md5.hexdigest()
    except (IOError, OSError):
        return None


def files_differ(path1: Path, path2: Path) -> bool:
    """
    æ¯”è¾ƒä¸¤ä¸ªæ–‡ä»¶æ˜¯å¦ä¸åŒï¼ˆå…ˆæ¯”è¾ƒå¤§å°ï¼Œå†æ¯”è¾ƒ MD5ï¼‰

    Args:
        path1: ç¬¬ä¸€ä¸ªæ–‡ä»¶è·¯å¾„
        path2: ç¬¬äºŒä¸ªæ–‡ä»¶è·¯å¾„

    Returns:
        bool: True è¡¨ç¤ºæ–‡ä»¶ä¸åŒï¼ŒFalse è¡¨ç¤ºç›¸åŒæˆ–ä»»ä¸€æ–‡ä»¶ä¸å­˜åœ¨
    """
    if not path1.exists() or not path2.exists():
        return False

    # å…ˆæ¯”è¾ƒå¤§å°
    size1 = path1.stat().st_size
    size2 = path2.stat().st_size
    if size1 != size2:
        return True

    # å¤§å°ç›¸åŒï¼Œæ¯”è¾ƒ MD5
    md5_1 = compute_md5(path1)
    md5_2 = compute_md5(path2)

    if md5_1 is None or md5_2 is None:
        return False

    return md5_1 != md5_2


def is_symlink(file_path: Path) -> bool:
    """æ£€æŸ¥æ˜¯å¦ä¸ºç¬¦å·é“¾æ¥"""
    return file_path.is_symlink()


# ============================================================================
# é…ç½®åŠ è½½
# ============================================================================

def parse_v2_config(config_path: Path) -> SyncConfig:
    """
    è§£æ v2.0 é…ç½®æ–‡ä»¶

    Args:
        config_path: é…ç½®æ–‡ä»¶è·¯å¾„

    Returns:
        SyncConfig: è§£æåçš„é…ç½®å¯¹è±¡
    """
    try:
        with open(config_path, 'r', encoding='utf-8') as f:
            data = json.load(f)

        config = SyncConfig()
        config.version = data.get('_version', '2.0.0')

        # è§£æ sync_directories - æ”¯æŒç®€å†™å’Œå®Œæ•´æ ¼å¼
        sync_dirs = data.get('sync_directories', [])
        if sync_dirs:
            for item in sync_dirs:
                if isinstance(item, str):
                    config.sync_directories.append({"path": item, "mode": "auto"})
                elif isinstance(item, dict):
                    config.sync_directories.append(item)

        # è§£æ exclude_directories
        exclude_dirs = data.get('exclude_directories', [])
        if exclude_dirs:
            for item in exclude_dirs:
                if isinstance(item, str):
                    config.exclude_directories.append({"path": item, "reason": ""})
                elif isinstance(item, dict):
                    config.exclude_directories.append(item)

        # è§£æåˆ é™¤åå•
        config.delete_list = data.get('delete_list', [])

        # è§£ææ–‡ä»¶é»‘åå•
        config.file_blacklist = data.get('file_blacklist', [])

        # è§£ææ–°æ–‡ä»¶ç­–ç•¥
        new_file_policy = data.get('new_file_policy', {})
        if isinstance(new_file_policy, bool):
            config.new_file_policy = {"enabled": new_file_policy}
        else:
            config.new_file_policy = new_file_policy

        # è§£ææœ¬åœ°è¦†ç›–é…ç½®
        config.local_overrides = data.get('local_overrides', {})

        # è§£æå¤‡ä»½é…ç½®
        config.backup = data.get('backup', {})

        return config

    except (json.JSONDecodeError, IOError) as e:
        print_error(f"è§£æé…ç½®æ–‡ä»¶å¤±è´¥: {e}")
        return get_default_config()


def parse_v1_config(config_path: Path) -> List[str]:
    """
    è§£æ v1.0 ç™½åå•é…ç½®æ–‡ä»¶

    Args:
        config_path: v1.0 é…ç½®æ–‡ä»¶è·¯å¾„

    Returns:
        List[str]: ç™½åå•æ¨¡å¼åˆ—è¡¨
    """
    try:
        with open(config_path, 'r', encoding='utf-8') as f:
            data = json.load(f)

        whitelist = []
        for _category, files in data.get('whitelist', {}).items():
            if isinstance(files, list):
                whitelist.extend(files)
            elif isinstance(files, str):
                whitelist.append(files)

        return whitelist
    except (json.JSONDecodeError, IOError):
        return []


def migrate_v1_to_v2(whitelist: List[str]) -> SyncConfig:
    """
    å°† v1.0 ç™½åå•è¿ç§»åˆ° v2.0 é…ç½®

    Args:
        whitelist: v1.0 ç™½åå•æ¨¡å¼åˆ—è¡¨

    Returns:
        SyncConfig: è¿ç§»åçš„ v2.0 é…ç½®
    """
    config = SyncConfig()

    # ä»ç™½åå•æ¨¡å¼ä¸­æå–ç›®å½•
    directories = set()
    for pattern in whitelist:
        # æå–ç›®å½•éƒ¨åˆ†
        if '/' in pattern:
            dir_part = pattern.split('/')[0]
            # å¤„ç†é€šé…ç¬¦
            if '*' not in dir_part:
                directories.add(dir_part)
            else:
                # å¦‚æœç›®å½•éƒ¨åˆ†æœ‰é€šé…ç¬¦ï¼Œå°è¯•æå–æ ¹ç›®å½•
                parts = dir_part.split('*')[0].rstrip('.')
                if parts:
                    directories.add(parts)

    # è½¬æ¢ä¸º sync_directories æ ¼å¼
    for directory in sorted(directories):
        config.sync_directories.append({
            "path": directory,
            "mode": "auto",
            "description": f"ä» v1.0 è¿ç§»: {directory}"
        })

    return config


def get_default_config() -> SyncConfig:
    """è·å–é»˜è®¤é…ç½®"""
    config = SyncConfig()
    config.sync_directories = [
        {"path": ".claude/agents", "mode": "auto"},
        {"path": ".claude/commands", "mode": "auto"},
        {"path": ".claude/hooks", "mode": "auto"},
        {"path": ".claude/skills", "mode": "auto"},
        {"path": ".specify/scripts", "mode": "auto"},
        {"path": ".specify/templates", "mode": "auto"}
    ]
    config.exclude_directories = [
        {"path": ".claude/commands/simplesdd", "reason": "SimpleSDD ä½¿ç”¨ä¸åŒçš„æ›´æ–°ç­–ç•¥"}
    ]
    config.file_blacklist = [
        "*.local.md",
        "*.backup.*",
        ".DS_Store",
        "Thumbs.db",
        "*.swp"
    ]
    config.new_file_policy = {"enabled": False}
    config.local_overrides = {"enabled": True}
    config.backup = {"enabled": True}
    return config


def load_config(repo_root: Path) -> Tuple[SyncConfig, str]:
    """
    åŠ è½½é…ç½®ï¼ŒæŒ‰ä¼˜å…ˆçº§æŸ¥æ‰¾

    ä¼˜å…ˆçº§ï¼š
    1. é¡¹ç›®é…ç½®: {repo_root}/.speckit-config.json
    2. L0 v2.0 é…ç½®: {l0}/update-config-v2.json
    3. v1.0 é…ç½®: {l0}/update-whitelist.jsonï¼ˆè‡ªåŠ¨è¿ç§»ï¼‰
    4. å†…ç½®é»˜è®¤é…ç½®

    Args:
        repo_root: ä»“åº“æ ¹ç›®å½•

    Returns:
        Tuple[SyncConfig, str]: (é…ç½®å¯¹è±¡, é…ç½®æ¥æºæè¿°)
    """
    script_dir = get_script_dir()

    # 1. å°è¯•åŠ è½½é¡¹ç›®é…ç½®
    project_config = repo_root / ".speckit-config.json"
    if project_config.exists():
        print_info(f"ä½¿ç”¨é¡¹ç›®é…ç½®: {project_config}")
        return parse_v2_config(project_config), "project"

    # 2. å°è¯•åŠ è½½ L0 v2.0 é…ç½®
    l0_v2_config = script_dir.parent / "update-config-v2.json"
    if l0_v2_config.exists():
        print_info(f"ä½¿ç”¨ L0 v2.0 é…ç½®: {l0_v2_config}")
        return parse_v2_config(l0_v2_config), "l0-v2"

    # 3. å°è¯•è¿ç§» v1.0 é…ç½®
    l0_v1_config = script_dir.parent / "update-whitelist.json"
    if l0_v1_config.exists():
        print_warning(f"æ£€æµ‹åˆ° v1.0 é…ç½®: {l0_v1_config}")
        print_warning("å·²è‡ªåŠ¨è¿ç§»åˆ° v2.0 æ ¼å¼")
        print_info("å»ºè®®åˆ›å»º .speckit-config.json ä½¿ç”¨æ–°æ ¼å¼")
        whitelist = parse_v1_config(l0_v1_config)
        return migrate_v1_to_v2(whitelist), "migrated"

    # 4. ä½¿ç”¨å†…ç½®é»˜è®¤é…ç½®
    print_warning("æœªæ‰¾åˆ°é…ç½®æ–‡ä»¶ï¼Œä½¿ç”¨é»˜è®¤é…ç½®")
    return get_default_config(), "default"


# ============================================================================
# ç›®å½•æ‰«æ
# ============================================================================

def scan_directory(
    directory: str,
    template_dir: Path,
    _repo_root: Path
) -> List[Path]:
    """
    æ‰«ææ¨¡æ¿ä»“åº“ä¸­çš„ç›®å½•ï¼Œæ”¶é›†æ‰€æœ‰æ–‡ä»¶

    Args:
        directory: è¦æ‰«æçš„ç›®å½•è·¯å¾„ï¼ˆç›¸å¯¹äºä»“åº“æ ¹ï¼‰
        template_dir: æ¨¡æ¿ä»“åº“æ ¹ç›®å½•
        repo_root: é¡¹ç›®ä»“åº“æ ¹ç›®å½•

    Returns:
        List[Path]: æ‰¾åˆ°çš„æ–‡ä»¶åˆ—è¡¨ï¼ˆç›¸å¯¹äºæ¨¡æ¿ç›®å½•çš„è·¯å¾„ï¼‰
    """
    files = []
    scan_path = template_dir / directory

    if not scan_path.exists():
        return files

    if scan_path.is_file():
        return [scan_path.relative_to(template_dir)]

    for root, _dirs, filenames in os.walk(scan_path):
        for filename in filenames:
            file_path = Path(root) / filename
            rel_path = file_path.relative_to(template_dir)
            files.append(rel_path)

    return files


def scan_local_files(
    directory: str,
    repo_root: Path
) -> List[Path]:
    """
    æ‰«ææœ¬åœ°é¡¹ç›®ç›®å½•ï¼Œæ”¶é›†æ‰€æœ‰æ–‡ä»¶

    Args:
        directory: è¦æ‰«æçš„ç›®å½•è·¯å¾„ï¼ˆç›¸å¯¹äºä»“åº“æ ¹ï¼‰
        repo_root: é¡¹ç›®ä»“åº“æ ¹ç›®å½•

    Returns:
        List[Path]: æ‰¾åˆ°çš„æ–‡ä»¶åˆ—è¡¨ï¼ˆç›¸å¯¹äºä»“åº“æ ¹çš„è·¯å¾„ï¼‰
    """
    files = []
    scan_path = repo_root / directory

    if not scan_path.exists():
        return files

    if scan_path.is_file():
        return [scan_path.relative_to(repo_root)]

    for root, _dirs, filenames in os.walk(scan_path):
        for filename in filenames:
            file_path = Path(root) / filename
            rel_path = file_path.relative_to(repo_root)
            files.append(rel_path)

    return files


def is_in_whitelist(rel_path: Path, sync_directories: List[Dict]) -> bool:
    """
    æ£€æŸ¥æ–‡ä»¶æ˜¯å¦åœ¨ç›®å½•ç™½åå•ä¸­

    Args:
        rel_path: æ–‡ä»¶çš„ç›¸å¯¹è·¯å¾„
        sync_directories: ç›®å½•ç™½åå•é…ç½®

    Returns:
        bool: æ˜¯å¦åœ¨ç™½åå•ä¸­
    """
    rel_path_str = normalize_path(str(rel_path))

    for dir_config in sync_directories:
        dir_path = dir_config.get("path", "")
        # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦åœ¨è¯¥ç›®å½•ä¸‹
        if rel_path_str.startswith(dir_path + "/") or rel_path_str == dir_path:
            return True

    return False


def is_in_excluded_directory(rel_path: Path, exclude_directories: List[Dict]) -> bool:
    """
    æ£€æŸ¥æ–‡ä»¶æ˜¯å¦åœ¨ç›®å½•é»‘åå•ä¸­

    Args:
        rel_path: æ–‡ä»¶çš„ç›¸å¯¹è·¯å¾„
        exclude_directories: ç›®å½•é»‘åå•é…ç½®

    Returns:
        bool: æ˜¯å¦åœ¨é»‘åå•ä¸­
    """
    rel_path_str = normalize_path(str(rel_path))

    for exclude_config in exclude_directories:
        exclude_path = exclude_config.get("path", "")
        if rel_path_str.startswith(exclude_path + "/") or rel_path_str == exclude_path:
            return True

    return False


def is_in_delete_list(rel_path: Path, delete_list: List[str]) -> bool:
    """
    æ£€æŸ¥æ–‡ä»¶æ˜¯å¦åœ¨åˆ é™¤åå•ä¸­

    Args:
        rel_path: æ–‡ä»¶çš„ç›¸å¯¹è·¯å¾„
        delete_list: åˆ é™¤åå•æ¨¡å¼åˆ—è¡¨

    Returns:
        bool: æ˜¯å¦åœ¨åˆ é™¤åå•ä¸­
    """
    rel_path_str = normalize_path(str(rel_path))
    return match_any_pattern(delete_list, rel_path_str)


def is_file_blacklisted(rel_path: Path, file_blacklist: List[str]) -> bool:
    """
    æ£€æŸ¥æ–‡ä»¶æ˜¯å¦åœ¨æ–‡ä»¶é»‘åå•ä¸­

    Args:
        rel_path: æ–‡ä»¶çš„ç›¸å¯¹è·¯å¾„
        file_blacklist: æ–‡ä»¶é»‘åå•æ¨¡å¼åˆ—è¡¨

    Returns:
        bool: æ˜¯å¦åœ¨é»‘åå•ä¸­
    """
    rel_path_str = normalize_path(str(rel_path))
    # æ£€æŸ¥å®Œæ•´è·¯å¾„
    if match_any_pattern(file_blacklist, rel_path_str):
        return True
    # æ£€æŸ¥æ–‡ä»¶å
    filename = rel_path.name
    for pattern in file_blacklist:
        if pattern.startswith("*.") or pattern.startswith("*"):
            if fnmatch.fnmatch(filename, pattern):
                return True
    return False


def is_local_override(rel_path: Path, repo_root: Path, local_overrides: Dict) -> bool:
    """
    æ£€æŸ¥æ–‡ä»¶æ˜¯å¦æœ‰æœ¬åœ°è¦†ç›–æ ‡è®°

    Args:
        rel_path: æ–‡ä»¶çš„ç›¸å¯¹è·¯å¾„
        repo_root: ä»“åº“æ ¹ç›®å½•
        local_overrides: æœ¬åœ°è¦†ç›–é…ç½®

    Returns:
        bool: æ˜¯å¦æœ‰æœ¬åœ°è¦†ç›–æ ‡è®°
    """
    if not local_overrides.get("enabled", True):
        return False

    file_path = repo_root / rel_path

    # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    if not file_path.exists():
        return False

    # æ–¹å¼1: æ£€æŸ¥æ ‡è®°æ–‡ä»¶
    marker_file = local_overrides.get("marker_file", ".speckit-local")
    marker_path = file_path.parent / marker_file
    if marker_path.exists():
        return True

    # æ–¹å¼2: æ£€æŸ¥æ–‡ä»¶å¤´æ ‡è®°
    header_markers = local_overrides.get("header_markers", [])
    if header_markers and file_path.is_file():
        try:
            # åªè¯»å–å‰å‡ è¡Œæ£€æŸ¥æ ‡è®°
            with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
                first_lines = [f.readline() for _ in range(min(5, 10))]
                content = ''.join(first_lines)
                for marker in header_markers:
                    if marker in content:
                        return True
        except (IOError, OSError):
            pass

    return False


# ============================================================================
# æ–‡ä»¶å‘ç°
# ============================================================================

def discover_files_to_sync(
    config: SyncConfig,
    template_dir: Path,
    repo_root: Path,
    verbose: bool = False
) -> Tuple[List[FileToSync], List[Tuple[str, str]]]:
    """
    å‘æ˜éœ€è¦åŒæ­¥çš„æ–‡ä»¶

    è¿‡æ»¤ä¼˜å…ˆçº§ï¼š
    1. ç›®å½•ç™½åå•ï¼ˆåªå¤„ç†è¿™äº›ç›®å½•ï¼‰
    2. ç›®å½•é»‘åå•ï¼ˆæ’é™¤å­ç›®å½•ï¼‰
    3. åˆ é™¤åå•ï¼ˆæ˜ç¡®æ’é™¤çš„æ–‡ä»¶ï¼‰
    4. æ–‡ä»¶é»‘åå•ï¼ˆé€šé…ç¬¦æ’é™¤ï¼‰
    5. æœ¬åœ°è¦†ç›–æ ‡è®°ï¼ˆæ–‡ä»¶å¤´æˆ–æ ‡è®°æ–‡ä»¶ï¼‰

    åˆ é™¤é€»è¾‘ï¼š
    - æ£€æŸ¥æœ¬åœ°æ˜¯å¦å­˜åœ¨åŒ¹é… delete_list çš„æ–‡ä»¶
    - å¦‚æœæ¨¡æ¿ä»“åº“ä¸­ä¸å­˜åœ¨å¯¹åº”æ–‡ä»¶ï¼Œåˆ™æ ‡è®°ä¸ºåˆ é™¤

    Args:
        config: åŒæ­¥é…ç½®
        template_dir: æ¨¡æ¿ä»“åº“æ ¹ç›®å½•
        repo_root: é¡¹ç›®ä»“åº“æ ¹ç›®å½•
        verbose: æ˜¯å¦æ˜¾ç¤ºè¯¦ç»†æ—¥å¿—

    Returns:
        Tuple[List[FileToSync], List[Tuple[str, str]]]: (éœ€è¦åŒæ­¥çš„æ–‡ä»¶åˆ—è¡¨, è¢«æ’é™¤çš„æ–‡ä»¶åˆ—è¡¨)
    """
    files_to_sync = []
    excluded_files = []

    # æ”¶é›†æ‰€æœ‰æ¨¡æ¿æ–‡ä»¶
    all_template_files = set()
    for dir_config in config.sync_directories:
        directory = dir_config.get("path", "")
        files = scan_directory(directory, template_dir, repo_root)
        all_template_files.update(files)

    print_verbose(f"åœ¨æ¨¡æ¿ç›®å½•ä¸­æ‰¾åˆ° {len(all_template_files)} ä¸ªæ–‡ä»¶", verbose)

    # è¿‡æ»¤æ–‡ä»¶
    for rel_path in sorted(all_template_files):
        src_path = template_dir / rel_path
        dst_path = repo_root / rel_path
        rel_path_str = normalize_path(str(rel_path))

        # è·³è¿‡ç¬¦å·é“¾æ¥
        if is_symlink(src_path):
            excluded_files.append((rel_path_str, ExclusionReason.SYMLINK))
            continue

        # 1. æ£€æŸ¥ç›®å½•ç™½åå•
        if not is_in_whitelist(rel_path, config.sync_directories):
            excluded_files.append((rel_path_str, ExclusionReason.NOT_IN_WHITELIST))
            continue

        # 2. æ£€æŸ¥ç›®å½•é»‘åå•
        if is_in_excluded_directory(rel_path, config.exclude_directories):
            excluded_files.append((rel_path_str, ExclusionReason.IN_EXCLUDE_DIR))
            continue

        # 3. æ£€æŸ¥åˆ é™¤åå•
        if is_in_delete_list(rel_path, config.delete_list):
            excluded_files.append((rel_path_str, ExclusionReason.IN_DELETE_LIST))
            continue

        # 4. æ£€æŸ¥æ–‡ä»¶é»‘åå•
        if is_file_blacklisted(rel_path, config.file_blacklist):
            excluded_files.append((rel_path_str, ExclusionReason.IN_BLACKLIST))
            continue

        # 5. æ£€æŸ¥æœ¬åœ°è¦†ç›–æ ‡è®°
        if is_local_override(rel_path, repo_root, config.local_overrides):
            excluded_files.append((rel_path_str, ExclusionReason.LOCAL_OVERRIDE))
            continue

        # ç¡®å®šæ“ä½œç±»å‹
        if not dst_path.exists():
            # æ–‡ä»¶ä¸å­˜åœ¨
            if config.new_file_policy.get("enabled", False):
                files_to_sync.append(FileToSync(
                    rel_path=rel_path_str,
                    action="create",
                    src=str(src_path),
                    dst=str(dst_path)
                ))
            else:
                excluded_files.append((rel_path_str, ExclusionReason.NOT_EXIST_NEW_POLICY))
        elif files_differ(src_path, dst_path):
            # æ–‡ä»¶æœ‰å·®å¼‚
            files_to_sync.append(FileToSync(
                rel_path=rel_path_str,
                action="update",
                src=str(src_path),
                dst=str(dst_path)
            ))
        else:
            # æ–‡ä»¶ç›¸åŒ
            excluded_files.append((rel_path_str, ExclusionReason.NO_DIFFERENCE))

    # åˆ é™¤é€»è¾‘ï¼šæ£€æŸ¥æœ¬åœ°åŒ¹é… delete_list çš„æ–‡ä»¶
    if config.delete_list:
        print_verbose(f"æ£€æŸ¥åˆ é™¤åå•ï¼Œå…± {len(config.delete_list)} é¡¹", verbose)

        # æ”¶é›†æ‰€æœ‰æœ¬åœ°æ–‡ä»¶ï¼ˆåœ¨åŒæ­¥ç›®å½•ä¸­ï¼‰
        all_local_files = set()
        for dir_config in config.sync_directories:
            directory = dir_config.get("path", "")
            files = scan_local_files(directory, repo_root)
            all_local_files.update(files)

        # å°†æ¨¡æ¿æ–‡ä»¶é›†åˆè½¬æ¢ä¸ºå­—ç¬¦ä¸²é›†åˆä¾¿äºæŸ¥æ‰¾
        template_files_set = {normalize_path(str(p)) for p in all_template_files}

        # é¢å¤–å¤„ç†ï¼šæ£€æŸ¥ delete_list ä¸­çš„ç²¾ç¡®è·¯å¾„ï¼ˆå¯èƒ½åœ¨åŒæ­¥ç›®å½•å¤–ï¼‰
        for pattern in config.delete_list:
            if '*' in pattern or '?' in pattern or '[' in pattern:
                # é€šé…ç¬¦æ¨¡å¼ï¼Œè·³è¿‡ï¼ˆåé¢åœ¨ all_local_files ä¸­å¤„ç†ï¼‰
                continue

            # ç²¾ç¡®è·¯å¾„æ¨¡å¼ï¼Œæ£€æŸ¥è¯¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
            exact_path = repo_root / pattern
            if exact_path.exists() and exact_path.is_file():
                rel_path = Path(pattern)
                all_local_files.add(rel_path)
                print_verbose(f"å‘ç°å¾…åˆ é™¤æ–‡ä»¶ï¼ˆç²¾ç¡®è·¯å¾„ï¼‰: {pattern}", verbose)

        for rel_path in sorted(all_local_files):
            rel_path_str = normalize_path(str(rel_path))
            local_path = repo_root / rel_path

            # æ£€æŸ¥æ˜¯å¦åœ¨åˆ é™¤åå•ä¸­
            if not is_in_delete_list(rel_path, config.delete_list):
                continue

            # æ£€æŸ¥æ¨¡æ¿ä»“åº“ä¸­æ˜¯å¦è¿˜å­˜åœ¨è¯¥æ–‡ä»¶
            if rel_path_str in template_files_set:
                # æ¨¡æ¿ä¸­è¿˜æœ‰è¿™ä¸ªæ–‡ä»¶ï¼Œä¸åˆ é™¤
                continue

            # æ£€æŸ¥æ˜¯å¦æœ‰æœ¬åœ°è¦†ç›–æ ‡è®°
            if is_local_override(rel_path, repo_root, config.local_overrides):
                excluded_files.append((rel_path_str, "æœ¬åœ°è¦†ç›–æ ‡è®°ï¼Œè·³è¿‡åˆ é™¤"))
                continue

            # æ ‡è®°ä¸ºå¾…åˆ é™¤
            files_to_sync.append(FileToSync(
                rel_path=rel_path_str,
                action="delete",
                src="",
                dst=str(local_path)
            ))

    return files_to_sync, excluded_files


# ============================================================================
# æ–‡ä»¶æ“ä½œ
# ============================================================================

def backup_file(file_path: Path, backup_dir: Path) -> bool:
    """
    å¤‡ä»½æ–‡ä»¶

    Args:
        file_path: è¦å¤‡ä»½çš„æ–‡ä»¶è·¯å¾„
        backup_dir: å¤‡ä»½ç›®å½•

    Returns:
        bool: æ˜¯å¦æˆåŠŸ
    """
    if not file_path.exists():
        return False

    try:
        backup_dir.mkdir(parents=True, exist_ok=True)
        rel_path = file_path.relative_to(file_path.anchor)
        backup_path = backup_dir / rel_path

        # ç¡®ä¿å¤‡ä»½ç›®å½•å­˜åœ¨
        backup_path.parent.mkdir(parents=True, exist_ok=True)

        # å¤åˆ¶æ–‡ä»¶
        shutil.copy2(file_path, backup_path)
        return True
    except (IOError, OSError):
        return False


def update_files(
    files_to_sync: List[FileToSync],
    config: SyncConfig,
    repo_root: Path,
    dry_run: bool = False,
    _verbose: bool = False
) -> SyncResult:
    """
    æ‰§è¡Œæ–‡ä»¶æ›´æ–°

    Args:
        files_to_sync: éœ€è¦åŒæ­¥çš„æ–‡ä»¶åˆ—è¡¨
        config: åŒæ­¥é…ç½®
        repo_root: ä»“åº“æ ¹ç›®å½•
        dry_run: æ˜¯å¦ä¸ºé¢„è§ˆæ¨¡å¼
        verbose: æ˜¯å¦æ˜¾ç¤ºè¯¦ç»†æ—¥å¿—

    Returns:
        SyncResult: åŒæ­¥ç»“æœ
    """
    result = SyncResult()

    # è®¾ç½®å¤‡ä»½ç›®å½•
    backup_enabled = config.backup.get("enabled", True) and not dry_run
    backup_dir: Optional[Path] = None
    if backup_enabled:
        backup_dir_name = config.backup.get("backup_dir", ".speckit-backup")
        backup_dir = repo_root / backup_dir_name

    for file_info in files_to_sync:
        src_path = Path(file_info.src)
        dst_path = Path(file_info.dst)

        try:
            # ç¡®ä¿ç›®æ ‡ç›®å½•å­˜åœ¨
            dst_path.parent.mkdir(parents=True, exist_ok=True)

            if file_info.action == "create":
                if dry_run:
                    print(f"  {Colors.CYAN}[CREATE]{Colors.NC} {file_info.rel_path}")
                    result.created += 1
                else:
                    if backup_enabled and dst_path.exists() and backup_dir:
                        backup_file(dst_path, backup_dir)
                    shutil.copy2(src_path, dst_path)
                    print_success(f"  [CREATE] {file_info.rel_path}")
                    result.created += 1

            elif file_info.action == "update":
                if dry_run:
                    print(f"  {Colors.GREEN}[UPDATE]{Colors.NC} {file_info.rel_path}")
                    result.updated += 1
                else:
                    if backup_enabled and backup_dir:
                        backup_file(dst_path, backup_dir)
                    shutil.copy2(src_path, dst_path)
                    print_success(f"  [UPDATE] {file_info.rel_path}")
                    result.updated += 1

            elif file_info.action == "delete":
                if dry_run:
                    print(f"  {Colors.RED}[DELETE]{Colors.NC} {file_info.rel_path}")
                    result.deleted += 1
                else:
                    if backup_enabled and dst_path.exists() and backup_dir:
                        backup_file(dst_path, backup_dir)
                    if dst_path.exists():
                        dst_path.unlink()
                        print_success(f"  [DELETE] {file_info.rel_path}")
                        result.deleted += 1
                    else:
                        print_warning(f"  [SKIP] {file_info.rel_path} (æ–‡ä»¶ä¸å­˜åœ¨)")

        except Exception as e:
            result.errors += 1
            print_error(f"  {file_info.rel_path}: {e}")

    return result


def clone_template_repo(temp_dir: Path, _verbose: bool = False) -> bool:
    """
    å…‹éš†æ¨¡æ¿ä»“åº“

    Args:
        temp_dir: ä¸´æ—¶ç›®å½•è·¯å¾„
        verbose: æ˜¯å¦æ˜¾ç¤ºè¯¦ç»†æ—¥å¿—

    Returns:
        bool: æ˜¯å¦æˆåŠŸ
    """
    template_repo = "git@github.com:WeTechHK/AI-SDD-template.git"
    branch = "main"

    print_info(f"æ­£åœ¨å…‹éš†æ¨¡æ¿ä»“åº“: {template_repo}")

    try:
        result = subprocess.run(
            ['git', 'clone', '--depth', '1', '--branch', branch, template_repo, str(temp_dir)],
            capture_output=True,
            text=True
        )
        if result.returncode != 0:
            print_error(f"å…‹éš†å¤±è´¥: {result.stderr}")
            return False
        print_success("æ¨¡æ¿ä»“åº“å…‹éš†å®Œæˆ")
        return True
    except Exception as e:
        print_error(f"å…‹éš†å¼‚å¸¸: {e}")
        return False


# ============================================================================
# è¾“å‡ºæ˜¾ç¤º
# ============================================================================

def print_summary(
    result: SyncResult,
    excluded_files: List[Tuple[str, str]],
    dry_run: bool = False
) -> None:
    """
    æ‰“å°æ‰§è¡Œæ‘˜è¦

    Args:
        result: åŒæ­¥ç»“æœ
        excluded_files: è¢«æ’é™¤çš„æ–‡ä»¶åˆ—è¡¨
        dry_run: æ˜¯å¦ä¸ºé¢„è§ˆæ¨¡å¼
    """
    print()
    print("=" * 60)
    if dry_run:
        print(f"é¢„è§ˆå®Œæˆ: {result.updated} ä¸ªå¾…æ›´æ–°, {result.created} ä¸ªå¾…åˆ›å»º, "
              f"{result.deleted} ä¸ªå¾…åˆ é™¤, {len(excluded_files)} ä¸ªå·²æ’é™¤")
    else:
        print(f"æ›´æ–°å®Œæˆ: {result.updated} ä¸ªå·²æ›´æ–°, {result.created} ä¸ªå·²åˆ›å»º, "
              f"{result.deleted} ä¸ªå·²åˆ é™¤, {result.skipped} ä¸ªå·²è·³è¿‡, "
              f"{len(excluded_files)} ä¸ªå·²æ’é™¤")
        if result.errors > 0:
            print(f"{Colors.RED}{result.errors} ä¸ªé”™è¯¯{Colors.NC}")
    print("=" * 60)
    print()


def print_excluded_files(excluded_files: List[Tuple[str, str]], show_excluded: bool) -> None:
    """
    æ‰“å°è¢«æ’é™¤çš„æ–‡ä»¶

    Args:
        excluded_files: è¢«æ’é™¤çš„æ–‡ä»¶åˆ—è¡¨ (path, reason)
        show_excluded: æ˜¯å¦æ˜¾ç¤ºè¢«æ’é™¤çš„æ–‡ä»¶
    """
    if not show_excluded or not excluded_files:
        return

    print()
    print(f"{Colors.CYAN}ğŸ“‹ è¢«æ’é™¤çš„æ–‡ä»¶:{Colors.NC}")
    print()

    # æŒ‰åŸå› åˆ†ç»„
    by_reason: Dict[str, List[str]] = {}
    for path, reason in excluded_files:
        if reason not in by_reason:
            by_reason[reason] = []
        by_reason[reason].append(path)

    for reason, paths in sorted(by_reason.items()):
        print(f"{Colors.YELLOW}æ’é™¤åŸå› : {reason}{Colors.NC}")
        for path in sorted(paths)[:20]:  # é™åˆ¶æ˜¾ç¤ºæ•°é‡
            print(f"  â€¢ {path}")
        if len(paths) > 20:
            print(f"  ... è¿˜æœ‰ {len(paths) - 20} ä¸ªæ–‡ä»¶")
        print()


# ============================================================================
# ä¸»å‡½æ•°
# ============================================================================

def main() -> None:
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(
        description='SpecKit å¢é‡æ›´æ–°è„šæœ¬ v2.0',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹:
  %(prog)s                    # æ‰§è¡Œæ›´æ–°
  %(prog)s --dry-run          # é¢„è§ˆæ¨¡å¼ï¼Œä¸å®é™…æ›´æ–°
  %(prog)s -v --show-excluded # æ˜¾ç¤ºè¯¦ç»†æ—¥å¿—å’Œè¢«æ’é™¤çš„æ–‡ä»¶
        """
    )
    parser.add_argument('--dry-run', '-n', action='store_true',
                        help='æ¨¡æ‹Ÿè¿è¡Œï¼Œä¸å®é™…æ›´æ–°æ–‡ä»¶')
    parser.add_argument('--verbose', '-v', action='store_true',
                        help='æ˜¾ç¤ºè¯¦ç»†ä¿¡æ¯')
    parser.add_argument('--show-excluded', action='store_true',
                        help='æ˜¾ç¤ºè¢«æ’é™¤çš„æ–‡ä»¶åŠåŸå› ')
    args = parser.parse_args()

    print()
    print("=" * 60)
    print(f"{Colors.BOLD}           SpecKit å¢é‡æ›´æ–° v2.0{Colors.NC}")
    print("=" * 60)
    print()

    # è·å–ä»“åº“æ ¹ç›®å½•
    repo_root = get_repo_root()
    os.chdir(repo_root)

    # åŠ è½½é…ç½®
    config, config_source = load_config(repo_root)
    print_info(f"é…ç½®ç‰ˆæœ¬: {config.version}")
    print_info(f"é…ç½®æ¥æº: {config_source}")
    print_info(f"åŒæ­¥ç›®å½•: {len(config.sync_directories)} ä¸ª")
    if config.delete_list:
        print_info(f"åˆ é™¤åå•: {len(config.delete_list)} é¡¹")
    print()

    # åˆ›å»ºä¸´æ—¶ç›®å½•
    with tempfile.TemporaryDirectory() as temp_dir:
        temp_path = Path(temp_dir) / "speckit-update"

        # å…‹éš†æ¨¡æ¿ä»“åº“
        if not clone_template_repo(temp_path, args.verbose):
            sys.exit(1)

        print()

        # å‘æ˜éœ€è¦åŒæ­¥çš„æ–‡ä»¶
        files_to_sync, excluded_files = discover_files_to_sync(
            config, temp_path, repo_root, args.verbose
        )

        print()
        print_info(f"æ‰¾åˆ° {len(files_to_sync)} ä¸ªæ–‡ä»¶å¾…å¤„ç†")
        print()

        if args.dry_run:
            print(f"{Colors.CYAN}ğŸ“‹ é¢„è§ˆæ›´æ–°æ¸…å•:{Colors.NC}")
        else:
            print(f"{Colors.GREEN}ğŸ“‹ æ›´æ–°æ¸…å•:{Colors.NC}")

        if not files_to_sync:
            print_warning("  æ²¡æœ‰æ–‡ä»¶éœ€è¦æ›´æ–°")
        else:
            # æ‰§è¡Œæ›´æ–°
            result = update_files(
                files_to_sync, config, repo_root, args.dry_run, args.verbose
            )
            result.excluded_files = excluded_files

        # æ˜¾ç¤ºè¢«æ’é™¤çš„æ–‡ä»¶
        print_excluded_files(excluded_files, args.show_excluded)

        # æ˜¾ç¤ºæ‘˜è¦
        print_summary(
            SyncResult(
                updated=len([f for f in files_to_sync if f.action == "update"]),
                created=len([f for f in files_to_sync if f.action == "create"]),
                deleted=len([f for f in files_to_sync if f.action == "delete"]),
                excluded=len(excluded_files)
            ),
            excluded_files,
            args.dry_run
        )


if __name__ == '__main__':
    main()
